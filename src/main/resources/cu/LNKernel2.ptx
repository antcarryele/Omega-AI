//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31442593
// Cuda compilation tools, release 11.7, V11.7.99
// Based on NVVM 7.0.1
//

.version 7.7
.target sm_52
.address_size 64

	// .globl	cuApplyLayerNorm
.extern .shared .align 16 .b8 buf[];

.visible .entry cuApplyLayerNorm(
	.param .u64 cuApplyLayerNorm_param_0,
	.param .u64 cuApplyLayerNorm_param_1,
	.param .u64 cuApplyLayerNorm_param_2,
	.param .u64 cuApplyLayerNorm_param_3,
	.param .u32 cuApplyLayerNorm_param_4,
	.param .u32 cuApplyLayerNorm_param_5,
	.param .f32 cuApplyLayerNorm_param_6,
	.param .u64 cuApplyLayerNorm_param_7,
	.param .u64 cuApplyLayerNorm_param_8
)
{
	.reg .pred 	%p<38>;
	.reg .f32 	%f<369>;
	.reg .b32 	%r<136>;
	.reg .b64 	%rd<82>;


	ld.param.u64 	%rd30, [cuApplyLayerNorm_param_0];
	ld.param.u64 	%rd27, [cuApplyLayerNorm_param_1];
	ld.param.u64 	%rd28, [cuApplyLayerNorm_param_2];
	ld.param.u64 	%rd29, [cuApplyLayerNorm_param_3];
	ld.param.u32 	%r42, [cuApplyLayerNorm_param_4];
	ld.param.u32 	%r43, [cuApplyLayerNorm_param_5];
	ld.param.u64 	%rd31, [cuApplyLayerNorm_param_7];
	ld.param.u64 	%rd32, [cuApplyLayerNorm_param_8];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd32;
	cvta.to.global.u64 	%rd3, %rd31;
	cvta.to.global.u64 	%rd4, %rd29;
	mov.u32 	%r127, %ctaid.y;
	setp.ge.s32 	%p1, %r127, %r42;
	@%p1 bra 	$L__BB0_50;

	mov.u32 	%r44, %ntid.x;
	mov.u32 	%r2, %ntid.y;
	mul.lo.s32 	%r45, %r44, %r2;
	mov.u32 	%r3, %tid.y;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r3, %r44, %r4;
	shl.b32 	%r6, %r5, 3;
	shl.b32 	%r7, %r45, 3;
	or.b32  	%r8, %r6, 1;
	shr.u32 	%r9, %r2, 1;
	cvt.rn.f32.s32 	%f1, %r43;
	or.b32  	%r10, %r4, %r3;
	shl.b32 	%r46, %r3, 3;
	mov.u32 	%r47, buf;
	add.s32 	%r11, %r47, %r46;
	add.s32 	%r48, %r2, %r3;
	shl.b32 	%r49, %r48, 2;
	add.s32 	%r12, %r47, %r49;
	add.s32 	%r50, %r45, %r43;
	add.s32 	%r14, %r5, %r45;
	not.b32 	%r51, %r14;
	add.s32 	%r52, %r50, %r51;
	div.u32 	%r15, %r52, %r45;
	add.s32 	%r53, %r15, 1;
	add.s32 	%r54, %r4, 1;
	and.b32  	%r16, %r54, 31;
	add.s32 	%r55, %r4, 2;
	and.b32  	%r17, %r55, 31;
	add.s32 	%r56, %r4, 4;
	and.b32  	%r18, %r56, 31;
	add.s32 	%r57, %r4, 8;
	and.b32  	%r19, %r57, 31;
	add.s32 	%r58, %r4, 16;
	and.b32  	%r20, %r58, 31;
	and.b32  	%r21, %r53, 3;
	cvt.s64.s32 	%rd5, %r5;
	mul.wide.s32 	%rd33, %r5, 4;
	add.s64 	%rd6, %rd3, %rd33;
	add.s64 	%rd7, %rd2, %rd33;
	cvt.s64.s32 	%rd8, %r14;
	cvt.s64.s32 	%rd9, %r45;
	mul.wide.s32 	%rd16, %r45, 4;
	add.s64 	%rd10, %rd6, %rd16;
	add.s64 	%rd11, %rd7, %rd16;
	add.s32 	%r22, %r14, %r45;
	cvt.s64.s32 	%rd12, %r22;
	add.s64 	%rd13, %rd10, %rd16;
	add.s64 	%rd14, %rd11, %rd16;
	add.s32 	%r23, %r22, %r45;
	cvta.to.global.u64 	%rd17, %rd27;
	cvta.to.global.u64 	%rd18, %rd28;
	mov.f32 	%f314, 0f00000000;
	setp.gt.u32 	%p17, %r2, 1;
	setp.eq.s32 	%p18, %r9, 0;
	setp.ne.s32 	%p29, %r10, 0;
	setp.ne.s32 	%p20, %r4, 0;

$L__BB0_2:
	shl.b32 	%r130, %r5, 3;
	ld.param.u64 	%rd79, [cuApplyLayerNorm_param_3];
	mul.lo.s32 	%r59, %r127, %r43;
	cvt.s64.s32 	%rd19, %r59;
	mul.wide.s32 	%rd34, %r59, 4;
	add.s64 	%rd20, %rd4, %rd34;
	add.s64 	%rd35, %rd79, %rd34;
	and.b64  	%rd36, %rd35, 3;
	setp.eq.s64 	%p2, %rd36, 0;
	mov.f32 	%f346, %f314;
	mov.f32 	%f347, %f314;
	mov.f32 	%f348, %f314;
	@%p2 bra 	$L__BB0_5;

	setp.ne.s32 	%p3, %r5, 0;
	mov.f32 	%f346, %f314;
	mov.f32 	%f347, %f314;
	mov.f32 	%f348, %f314;
	mov.u32 	%r130, %r8;
	@%p3 bra 	$L__BB0_5;

	ld.global.f32 	%f107, [%rd20];
	add.f32 	%f347, %f107, 0f00000000;
	sub.f32 	%f108, %f107, %f347;
	fma.rn.f32 	%f348, %f107, %f108, 0f00000000;
	mov.f32 	%f346, 0f3F800000;
	mov.u32 	%r130, %r8;

$L__BB0_5:
	add.s32 	%r60, %r130, 7;
	setp.ge.s32 	%p4, %r60, %r43;
	@%p4 bra 	$L__BB0_8;

$L__BB0_7:
	mul.wide.s32 	%rd37, %r130, 4;
	add.s64 	%rd38, %rd20, %rd37;
	ld.global.f32 	%f109, [%rd38];
	sub.f32 	%f110, %f109, %f347;
	add.f32 	%f111, %f346, 0f3F800000;
	div.rn.f32 	%f112, %f110, %f111;
	add.f32 	%f113, %f347, %f112;
	sub.f32 	%f114, %f109, %f113;
	fma.rn.f32 	%f115, %f110, %f114, %f348;
	add.f32 	%f116, %f111, 0f3F800000;
	ld.global.f32 	%f117, [%rd38+8];
	sub.f32 	%f118, %f117, %f113;
	div.rn.f32 	%f119, %f118, %f116;
	add.f32 	%f120, %f113, %f119;
	sub.f32 	%f121, %f117, %f120;
	fma.rn.f32 	%f122, %f118, %f121, %f115;
	add.f32 	%f123, %f116, 0f3F800000;
	ld.global.f32 	%f124, [%rd38+16];
	sub.f32 	%f125, %f124, %f120;
	div.rn.f32 	%f126, %f125, %f123;
	add.f32 	%f127, %f120, %f126;
	sub.f32 	%f128, %f124, %f127;
	fma.rn.f32 	%f129, %f125, %f128, %f122;
	add.f32 	%f346, %f123, 0f3F800000;
	ld.global.f32 	%f130, [%rd38+24];
	sub.f32 	%f131, %f130, %f127;
	div.rn.f32 	%f132, %f131, %f346;
	add.f32 	%f347, %f127, %f132;
	sub.f32 	%f133, %f130, %f347;
	fma.rn.f32 	%f348, %f131, %f133, %f129;
	add.s32 	%r130, %r130, %r7;
	add.s32 	%r61, %r130, 7;
	setp.lt.s32 	%p5, %r61, %r43;
	@%p5 bra 	$L__BB0_7;

$L__BB0_8:
	setp.ge.s32 	%p6, %r130, %r43;
	@%p6 bra 	$L__BB0_16;

	sub.s32 	%r62, %r43, %r130;
	and.b32  	%r29, %r62, 3;
	setp.eq.s32 	%p7, %r29, 0;
	mov.u32 	%r131, %r130;
	@%p7 bra 	$L__BB0_13;

	mul.wide.s32 	%rd39, %r130, 4;
	add.s64 	%rd21, %rd20, %rd39;
	ld.global.f32 	%f135, [%rd21];
	sub.f32 	%f136, %f135, %f347;
	add.f32 	%f346, %f346, 0f3F800000;
	div.rn.f32 	%f137, %f136, %f346;
	add.f32 	%f347, %f347, %f137;
	sub.f32 	%f138, %f135, %f347;
	fma.rn.f32 	%f348, %f136, %f138, %f348;
	add.s32 	%r131, %r130, 1;
	setp.eq.s32 	%p8, %r29, 1;
	@%p8 bra 	$L__BB0_13;

	sub.s32 	%r121, %r43, %r130;
	and.b32  	%r120, %r121, 3;
	ld.global.f32 	%f139, [%rd21+4];
	sub.f32 	%f140, %f139, %f347;
	add.f32 	%f346, %f346, 0f3F800000;
	div.rn.f32 	%f141, %f140, %f346;
	add.f32 	%f347, %f347, %f141;
	sub.f32 	%f142, %f139, %f347;
	fma.rn.f32 	%f348, %f140, %f142, %f348;
	add.s32 	%r131, %r130, 2;
	setp.eq.s32 	%p9, %r120, 2;
	@%p9 bra 	$L__BB0_13;

	ld.global.f32 	%f143, [%rd21+8];
	sub.f32 	%f144, %f143, %f347;
	add.f32 	%f346, %f346, 0f3F800000;
	div.rn.f32 	%f145, %f144, %f346;
	add.f32 	%f347, %f347, %f145;
	sub.f32 	%f146, %f143, %f347;
	fma.rn.f32 	%f348, %f144, %f146, %f348;
	add.s32 	%r131, %r130, 3;

$L__BB0_13:
	not.b32 	%r63, %r130;
	add.s32 	%r64, %r63, %r43;
	setp.lt.u32 	%p10, %r64, 3;
	@%p10 bra 	$L__BB0_16;

	add.s64 	%rd80, %rd4, 8;
	cvt.s64.s32 	%rd40, %r131;
	add.s64 	%rd41, %rd40, %rd19;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd81, %rd80, %rd42;

$L__BB0_15:
	ld.global.f32 	%f147, [%rd81+-8];
	sub.f32 	%f148, %f147, %f347;
	add.f32 	%f149, %f346, 0f3F800000;
	div.rn.f32 	%f150, %f148, %f149;
	add.f32 	%f151, %f347, %f150;
	sub.f32 	%f152, %f147, %f151;
	fma.rn.f32 	%f153, %f148, %f152, %f348;
	add.f32 	%f154, %f149, 0f3F800000;
	ld.global.f32 	%f155, [%rd81+-4];
	sub.f32 	%f156, %f155, %f151;
	div.rn.f32 	%f157, %f156, %f154;
	add.f32 	%f158, %f151, %f157;
	sub.f32 	%f159, %f155, %f158;
	fma.rn.f32 	%f160, %f156, %f159, %f153;
	add.f32 	%f161, %f154, 0f3F800000;
	ld.global.f32 	%f162, [%rd81];
	sub.f32 	%f163, %f162, %f158;
	div.rn.f32 	%f164, %f163, %f161;
	add.f32 	%f165, %f158, %f164;
	sub.f32 	%f166, %f162, %f165;
	fma.rn.f32 	%f167, %f163, %f166, %f160;
	add.f32 	%f346, %f161, 0f3F800000;
	ld.global.f32 	%f168, [%rd81+4];
	sub.f32 	%f169, %f168, %f165;
	div.rn.f32 	%f170, %f169, %f346;
	add.f32 	%f347, %f165, %f170;
	sub.f32 	%f171, %f168, %f347;
	fma.rn.f32 	%f348, %f169, %f171, %f167;
	add.s64 	%rd81, %rd81, 16;
	add.s32 	%r131, %r131, 4;
	setp.lt.s32 	%p11, %r131, %r43;
	@%p11 bra 	$L__BB0_15;

$L__BB0_16:
	mov.u32 	%r70, 31;
	// begin inline asm
	shfl.idx.b32 %f172, %f348, %r16, %r70;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f174, %f347, %r16, %r70;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f176, %f346, %r16, %r70;
	// end inline asm
	add.f32 	%f43, %f346, %f176;
	setp.leu.f32 	%p12, %f43, 0f00000000;
	mov.f32 	%f349, 0f00000000;
	mov.f32 	%f350, %f349;
	@%p12 bra 	$L__BB0_18;

	add.f32 	%f324, %f346, %f176;
	sub.f32 	%f180, %f174, %f347;
	div.rn.f32 	%f181, %f346, %f324;
	div.rn.f32 	%f182, %f176, %f324;
	mul.f32 	%f183, %f174, %f182;
	fma.rn.f32 	%f349, %f347, %f181, %f183;
	mul.f32 	%f184, %f180, %f180;
	mul.f32 	%f185, %f184, %f181;
	mul.f32 	%f186, %f182, %f185;
	add.f32 	%f187, %f348, %f172;
	fma.rn.f32 	%f350, %f324, %f186, %f187;

$L__BB0_18:
	mov.f32 	%f351, 0f00000000;
	mov.u32 	%r122, 31;
	add.f32 	%f318, %f346, %f176;
	// begin inline asm
	shfl.idx.b32 %f188, %f350, %r17, %r122;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f190, %f349, %r17, %r122;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f192, %f318, %r17, %r122;
	// end inline asm
	add.f32 	%f51, %f318, %f192;
	setp.leu.f32 	%p13, %f51, 0f00000000;
	mov.f32 	%f352, %f351;
	@%p13 bra 	$L__BB0_20;

	add.f32 	%f323, %f346, %f176;
	sub.f32 	%f196, %f190, %f349;
	div.rn.f32 	%f197, %f323, %f51;
	div.rn.f32 	%f198, %f192, %f51;
	mul.f32 	%f199, %f190, %f198;
	fma.rn.f32 	%f351, %f349, %f197, %f199;
	mul.f32 	%f200, %f196, %f196;
	mul.f32 	%f201, %f200, %f197;
	mul.f32 	%f202, %f198, %f201;
	add.f32 	%f203, %f350, %f188;
	fma.rn.f32 	%f352, %f51, %f202, %f203;

$L__BB0_20:
	mov.f32 	%f353, 0f00000000;
	mov.u32 	%r123, 31;
	// begin inline asm
	shfl.idx.b32 %f204, %f352, %r18, %r123;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f206, %f351, %r18, %r123;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f208, %f51, %r18, %r123;
	// end inline asm
	add.f32 	%f59, %f51, %f208;
	setp.leu.f32 	%p14, %f59, 0f00000000;
	mov.f32 	%f354, %f353;
	@%p14 bra 	$L__BB0_22;

	sub.f32 	%f212, %f206, %f351;
	div.rn.f32 	%f213, %f51, %f59;
	div.rn.f32 	%f214, %f208, %f59;
	mul.f32 	%f215, %f206, %f214;
	fma.rn.f32 	%f353, %f351, %f213, %f215;
	mul.f32 	%f216, %f212, %f212;
	mul.f32 	%f217, %f216, %f213;
	mul.f32 	%f218, %f214, %f217;
	add.f32 	%f219, %f352, %f204;
	fma.rn.f32 	%f354, %f59, %f218, %f219;

$L__BB0_22:
	mov.f32 	%f355, 0f00000000;
	mov.u32 	%r124, 31;
	// begin inline asm
	shfl.idx.b32 %f220, %f354, %r19, %r124;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f222, %f353, %r19, %r124;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f224, %f59, %r19, %r124;
	// end inline asm
	add.f32 	%f67, %f59, %f224;
	setp.leu.f32 	%p15, %f67, 0f00000000;
	mov.f32 	%f356, %f355;
	@%p15 bra 	$L__BB0_24;

	add.f32 	%f327, %f59, %f224;
	sub.f32 	%f228, %f222, %f353;
	div.rn.f32 	%f229, %f59, %f327;
	div.rn.f32 	%f230, %f224, %f327;
	mul.f32 	%f231, %f222, %f230;
	fma.rn.f32 	%f355, %f353, %f229, %f231;
	mul.f32 	%f232, %f228, %f228;
	mul.f32 	%f233, %f232, %f229;
	mul.f32 	%f234, %f230, %f233;
	add.f32 	%f235, %f354, %f220;
	fma.rn.f32 	%f356, %f327, %f234, %f235;

$L__BB0_24:
	add.f32 	%f325, %f59, %f224;
	mov.f32 	%f363, 0f00000000;
	mov.u32 	%r125, 31;
	// begin inline asm
	shfl.idx.b32 %f236, %f356, %r20, %r125;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f238, %f355, %r20, %r125;
	// end inline asm
	// begin inline asm
	shfl.idx.b32 %f240, %f325, %r20, %r125;
	// end inline asm
	add.f32 	%f362, %f325, %f240;
	setp.leu.f32 	%p16, %f362, 0f00000000;
	mov.f32 	%f364, %f363;
	@%p16 bra 	$L__BB0_26;

	add.f32 	%f326, %f59, %f224;
	sub.f32 	%f244, %f238, %f355;
	div.rn.f32 	%f245, %f326, %f362;
	div.rn.f32 	%f246, %f240, %f362;
	mul.f32 	%f247, %f238, %f246;
	fma.rn.f32 	%f363, %f355, %f245, %f247;
	mul.f32 	%f248, %f244, %f244;
	mul.f32 	%f249, %f248, %f245;
	mul.f32 	%f250, %f246, %f249;
	add.f32 	%f251, %f356, %f236;
	fma.rn.f32 	%f364, %f362, %f250, %f251;

$L__BB0_26:
	@%p17 bra 	$L__BB0_28;
	bra.uni 	$L__BB0_27;

$L__BB0_28:
	@%p18 bra 	$L__BB0_36;

	mov.u32 	%r117, %ntid.y;
	shr.u32 	%r133, %r117, 1;

$L__BB0_30:
	mov.f32 	%f84, %f362;
	mov.f32 	%f83, %f363;
	mov.f32 	%f82, %f364;
	mov.u32 	%r36, %r133;
	setp.lt.u32 	%p19, %r3, %r36;
	or.pred  	%p21, %p20, %p19;
	shl.b32 	%r99, %r36, 1;
	setp.ge.u32 	%p22, %r3, %r99;
	or.pred  	%p23, %p21, %p22;
	@%p23 bra 	$L__BB0_32;

	mov.u32 	%r119, %ntid.y;
	mov.u32 	%r118, buf;
	sub.s32 	%r100, %r3, %r36;
	shl.b32 	%r101, %r100, 3;
	add.s32 	%r103, %r118, %r101;
	st.shared.v2.f32 	[%r103], {%f83, %f82};
	add.s32 	%r104, %r100, %r119;
	shl.b32 	%r105, %r104, 2;
	add.s32 	%r106, %r118, %r105;
	st.shared.f32 	[%r106], %f84;

$L__BB0_32:
	bar.sync 	0;
	setp.ge.u32 	%p25, %r3, %r36;
	or.pred  	%p26, %p20, %p25;
	mov.f32 	%f362, %f84;
	mov.f32 	%f363, %f83;
	mov.f32 	%f364, %f82;
	@%p26 bra 	$L__BB0_35;

	ld.shared.f32 	%f85, [%r12];
	add.f32 	%f362, %f84, %f85;
	setp.leu.f32 	%p27, %f362, 0f00000000;
	mov.f32 	%f363, 0f00000000;
	mov.f32 	%f364, %f363;
	@%p27 bra 	$L__BB0_35;

	ld.shared.f32 	%f258, [%r11];
	sub.f32 	%f259, %f258, %f83;
	div.rn.f32 	%f260, %f84, %f362;
	div.rn.f32 	%f261, %f85, %f362;
	mul.f32 	%f262, %f258, %f261;
	fma.rn.f32 	%f363, %f83, %f260, %f262;
	ld.shared.f32 	%f263, [%r11+4];
	add.f32 	%f264, %f82, %f263;
	mul.f32 	%f265, %f259, %f259;
	mul.f32 	%f266, %f265, %f260;
	mul.f32 	%f267, %f261, %f266;
	fma.rn.f32 	%f364, %f362, %f267, %f264;

$L__BB0_35:
	bar.sync 	0;
	shr.u32 	%r107, %r36, 31;
	add.s32 	%r108, %r36, %r107;
	shr.s32 	%r133, %r108, 1;
	setp.gt.s32 	%p28, %r36, 1;
	@%p28 bra 	$L__BB0_30;

$L__BB0_36:
	@%p29 bra 	$L__BB0_38;

	st.shared.v2.f32 	[buf], {%f363, %f364};

$L__BB0_38:
	bar.sync 	0;
	ld.shared.v2.f32 	{%f367, %f269}, [buf];
	div.rn.f32 	%f368, %f269, %f1;
	bra.uni 	$L__BB0_39;

$L__BB0_27:
	mov.u32 	%r126, 31;
	mov.u32 	%r97, 0;
	// begin inline asm
	shfl.idx.b32 %f367, %f363, %r97, %r126;
	// end inline asm
	div.rn.f32 	%f255, %f364, %f1;
	// begin inline asm
	shfl.idx.b32 %f368, %f255, %r97, %r126;
	// end inline asm

$L__BB0_39:
	ld.param.f32 	%f317, [cuApplyLayerNorm_param_6];
	add.f32 	%f271, %f368, %f317;
	rsqrt.approx.f32 	%f98, %f271;
	setp.ge.s32 	%p30, %r5, %r43;
	@%p30 bra 	$L__BB0_47;

	setp.eq.s32 	%p31, %r21, 0;
	mov.u32 	%r134, %r5;
	@%p31 bra 	$L__BB0_44;

	setp.eq.s32 	%p32, %r21, 1;
	ld.global.nc.f32 	%f272, [%rd6];
	mul.f32 	%f273, %f98, %f272;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd25, %rd20, %rd43;
	ld.global.f32 	%f274, [%rd25];
	sub.f32 	%f275, %f274, %f367;
	ld.global.nc.f32 	%f276, [%rd7];
	fma.rn.f32 	%f277, %f275, %f273, %f276;
	add.s64 	%rd44, %rd5, %rd19;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd1, %rd45;
	st.global.f32 	[%rd46], %f277;
	mov.u32 	%r134, %r14;
	@%p32 bra 	$L__BB0_44;

	setp.eq.s32 	%p33, %r21, 2;
	ld.global.nc.f32 	%f278, [%rd10];
	mul.f32 	%f279, %f98, %f278;
	shl.b64 	%rd47, %rd9, 2;
	add.s64 	%rd26, %rd25, %rd47;
	ld.global.f32 	%f280, [%rd26];
	sub.f32 	%f281, %f280, %f367;
	ld.global.nc.f32 	%f282, [%rd11];
	fma.rn.f32 	%f283, %f281, %f279, %f282;
	add.s64 	%rd48, %rd8, %rd19;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd1, %rd49;
	st.global.f32 	[%rd50], %f283;
	mov.u32 	%r134, %r22;
	@%p33 bra 	$L__BB0_44;

	add.s64 	%rd51, %rd12, %rd19;
	ld.global.nc.f32 	%f284, [%rd13];
	mul.f32 	%f285, %f98, %f284;
	add.s64 	%rd53, %rd26, %rd47;
	ld.global.f32 	%f286, [%rd53];
	sub.f32 	%f287, %f286, %f367;
	ld.global.nc.f32 	%f288, [%rd14];
	fma.rn.f32 	%f289, %f287, %f285, %f288;
	shl.b64 	%rd54, %rd51, 2;
	add.s64 	%rd55, %rd1, %rd54;
	st.global.f32 	[%rd55], %f289;
	mov.u32 	%r134, %r23;

$L__BB0_44:
	setp.lt.u32 	%p34, %r15, 3;
	@%p34 bra 	$L__BB0_47;

$L__BB0_46:
	cvt.s64.s32 	%rd56, %r134;
	add.s64 	%rd57, %rd56, %rd19;
	mul.wide.s32 	%rd58, %r134, 4;
	add.s64 	%rd59, %rd20, %rd58;
	add.s64 	%rd60, %rd3, %rd58;
	ld.global.nc.f32 	%f290, [%rd60];
	mul.f32 	%f291, %f98, %f290;
	ld.global.f32 	%f292, [%rd59];
	sub.f32 	%f293, %f292, %f367;
	add.s64 	%rd61, %rd2, %rd58;
	ld.global.nc.f32 	%f294, [%rd61];
	fma.rn.f32 	%f295, %f293, %f291, %f294;
	shl.b64 	%rd62, %rd57, 2;
	add.s64 	%rd63, %rd1, %rd62;
	st.global.f32 	[%rd63], %f295;
	cvt.u32.u64 	%r109, %rd9;
	add.s32 	%r110, %r134, %r109;
	add.s64 	%rd64, %rd59, %rd16;
	add.s64 	%rd65, %rd60, %rd16;
	ld.global.nc.f32 	%f296, [%rd65];
	mul.f32 	%f297, %f98, %f296;
	ld.global.f32 	%f298, [%rd64];
	sub.f32 	%f299, %f298, %f367;
	add.s64 	%rd66, %rd61, %rd16;
	ld.global.nc.f32 	%f300, [%rd66];
	fma.rn.f32 	%f301, %f299, %f297, %f300;
	add.s64 	%rd67, %rd63, %rd16;
	st.global.f32 	[%rd67], %f301;
	add.s32 	%r111, %r110, %r109;
	add.s64 	%rd68, %rd64, %rd16;
	add.s64 	%rd69, %rd65, %rd16;
	ld.global.nc.f32 	%f302, [%rd69];
	mul.f32 	%f303, %f98, %f302;
	ld.global.f32 	%f304, [%rd68];
	sub.f32 	%f305, %f304, %f367;
	add.s64 	%rd70, %rd66, %rd16;
	ld.global.nc.f32 	%f306, [%rd70];
	fma.rn.f32 	%f307, %f305, %f303, %f306;
	add.s64 	%rd71, %rd67, %rd16;
	st.global.f32 	[%rd71], %f307;
	add.s32 	%r112, %r111, %r109;
	add.s64 	%rd72, %rd68, %rd16;
	add.s64 	%rd73, %rd69, %rd16;
	ld.global.nc.f32 	%f308, [%rd73];
	mul.f32 	%f309, %f98, %f308;
	ld.global.f32 	%f310, [%rd72];
	sub.f32 	%f311, %f310, %f367;
	add.s64 	%rd74, %rd70, %rd16;
	ld.global.nc.f32 	%f312, [%rd74];
	fma.rn.f32 	%f313, %f311, %f309, %f312;
	add.s64 	%rd75, %rd71, %rd16;
	st.global.f32 	[%rd75], %f313;
	add.s32 	%r134, %r112, %r109;
	setp.lt.s32 	%p35, %r134, %r43;
	@%p35 bra 	$L__BB0_46;

$L__BB0_47:
	@%p29 bra 	$L__BB0_49;

	mul.wide.s32 	%rd76, %r127, 4;
	add.s64 	%rd77, %rd17, %rd76;
	st.global.f32 	[%rd77], %f367;
	add.s64 	%rd78, %rd18, %rd76;
	st.global.f32 	[%rd78], %f98;

$L__BB0_49:
	ld.param.u32 	%r114, [cuApplyLayerNorm_param_4];
	mov.u32 	%r113, %nctaid.y;
	bar.sync 	0;
	add.s32 	%r127, %r127, %r113;
	setp.lt.s32 	%p37, %r127, %r114;
	@%p37 bra 	$L__BB0_2;

$L__BB0_50:
	ret;

}

