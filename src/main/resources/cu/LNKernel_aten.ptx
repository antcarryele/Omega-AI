//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31442593
// Cuda compilation tools, release 11.7, V11.7.99
// Based on NVVM 7.0.1
//

.version 7.7
.target sm_52
.address_size 64

	// .globl	ComputeSigmaAndFusedParamsCUDAKernel
// _ZZ24RowwiseMomentsCUDAKernelE9m_storage has been demoted
// _ZZ24RowwiseMomentsCUDAKernelE9v_storage has been demoted
// _ZZ34ComputeInternalGradientsCUDAKernelE10ds_storage has been demoted
// _ZZ34ComputeInternalGradientsCUDAKernelE10db_storage has been demoted

.visible .entry ComputeSigmaAndFusedParamsCUDAKernel(
	.param .u32 ComputeSigmaAndFusedParamsCUDAKernel_param_0,
	.param .f32 ComputeSigmaAndFusedParamsCUDAKernel_param_1,
	.param .u64 ComputeSigmaAndFusedParamsCUDAKernel_param_2,
	.param .u64 ComputeSigmaAndFusedParamsCUDAKernel_param_3,
	.param .u64 ComputeSigmaAndFusedParamsCUDAKernel_param_4,
	.param .u64 ComputeSigmaAndFusedParamsCUDAKernel_param_5,
	.param .u64 ComputeSigmaAndFusedParamsCUDAKernel_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<17>;


	ld.param.u32 	%r2, [ComputeSigmaAndFusedParamsCUDAKernel_param_0];
	ld.param.f32 	%f1, [ComputeSigmaAndFusedParamsCUDAKernel_param_1];
	ld.param.u64 	%rd1, [ComputeSigmaAndFusedParamsCUDAKernel_param_2];
	ld.param.u64 	%rd2, [ComputeSigmaAndFusedParamsCUDAKernel_param_3];
	ld.param.u64 	%rd3, [ComputeSigmaAndFusedParamsCUDAKernel_param_4];
	ld.param.u64 	%rd4, [ComputeSigmaAndFusedParamsCUDAKernel_param_5];
	ld.param.u64 	%rd5, [ComputeSigmaAndFusedParamsCUDAKernel_param_6];
	mov.u32 	%r3, %ctaid.x;
	shl.b32 	%r4, %r3, 7;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r1, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f2, [%rd8];
	add.f32 	%f3, %f2, %f1;
	rsqrt.approx.f32 	%f4, %f3;
	mul.f32 	%f5, %f4, %f3;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd7;
	st.global.f32 	[%rd10], %f5;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	st.global.f32 	[%rd12], %f4;
	cvta.to.global.u64 	%rd13, %rd1;
	add.s64 	%rd14, %rd13, %rd7;
	ld.global.f32 	%f6, [%rd14];
	mul.f32 	%f7, %f4, %f6;
	neg.f32 	%f8, %f7;
	cvta.to.global.u64 	%rd15, %rd5;
	add.s64 	%rd16, %rd15, %rd7;
	st.global.f32 	[%rd16], %f8;

$L__BB0_2:
	ret;

}
	// .globl	LayerNormForwardCUDAKernel
.visible .entry LayerNormForwardCUDAKernel(
	.param .u32 LayerNormForwardCUDAKernel_param_0,
	.param .u32 LayerNormForwardCUDAKernel_param_1,
	.param .u64 LayerNormForwardCUDAKernel_param_2,
	.param .u64 LayerNormForwardCUDAKernel_param_3,
	.param .u64 LayerNormForwardCUDAKernel_param_4,
	.param .u64 LayerNormForwardCUDAKernel_param_5,
	.param .u64 LayerNormForwardCUDAKernel_param_6,
	.param .u64 LayerNormForwardCUDAKernel_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<22>;


	ld.param.u32 	%r3, [LayerNormForwardCUDAKernel_param_0];
	ld.param.u32 	%r2, [LayerNormForwardCUDAKernel_param_1];
	ld.param.u64 	%rd1, [LayerNormForwardCUDAKernel_param_2];
	ld.param.u64 	%rd2, [LayerNormForwardCUDAKernel_param_3];
	ld.param.u64 	%rd3, [LayerNormForwardCUDAKernel_param_4];
	ld.param.u64 	%rd4, [LayerNormForwardCUDAKernel_param_5];
	ld.param.u64 	%rd5, [LayerNormForwardCUDAKernel_param_6];
	ld.param.u64 	%rd6, [LayerNormForwardCUDAKernel_param_7];
	mov.u32 	%r4, %ctaid.x;
	shl.b32 	%r5, %r4, 7;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r1, %r5, %r6;
	mul.lo.s32 	%r7, %r2, %r3;
	setp.ge.s32 	%p1, %r1, %r7;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd7, %rd1;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	div.s32 	%r8, %r1, %r2;
	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.s32 	%rd11, %r8, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f32 	%f1, [%rd12];
	ld.global.f32 	%f2, [%rd9];
	cvta.to.global.u64 	%rd13, %rd3;
	add.s64 	%rd14, %rd13, %rd11;
	ld.global.f32 	%f3, [%rd14];
	fma.rn.f32 	%f4, %f2, %f1, %f3;
	mul.lo.s32 	%r9, %r8, %r2;
	sub.s32 	%r10, %r1, %r9;
	cvta.to.global.u64 	%rd15, %rd4;
	mul.wide.s32 	%rd16, %r10, 4;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.f32 	%f5, [%rd17];
	cvta.to.global.u64 	%rd18, %rd5;
	add.s64 	%rd19, %rd18, %rd16;
	ld.global.f32 	%f6, [%rd19];
	fma.rn.f32 	%f7, %f4, %f5, %f6;
	cvta.to.global.u64 	%rd20, %rd6;
	add.s64 	%rd21, %rd20, %rd8;
	st.global.f32 	[%rd21], %f7;

$L__BB1_2:
	ret;

}
	// .globl	RowwiseMomentsCUDAKernel
.visible .entry RowwiseMomentsCUDAKernel(
	.param .u32 RowwiseMomentsCUDAKernel_param_0,
	.param .u64 RowwiseMomentsCUDAKernel_param_1,
	.param .u64 RowwiseMomentsCUDAKernel_param_2,
	.param .u64 RowwiseMomentsCUDAKernel_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<72>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<13>;
	// demoted variable
	.shared .align 4 .b8 _ZZ24RowwiseMomentsCUDAKernelE9m_storage[24];
	// demoted variable
	.shared .align 4 .b8 _ZZ24RowwiseMomentsCUDAKernelE9v_storage[24];

	ld.param.u32 	%r8, [RowwiseMomentsCUDAKernel_param_0];
	ld.param.u64 	%rd1, [RowwiseMomentsCUDAKernel_param_1];
	ld.param.u64 	%rd2, [RowwiseMomentsCUDAKernel_param_2];
	ld.param.u64 	%rd3, [RowwiseMomentsCUDAKernel_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r2, %r8;
	mov.f32 	%f68, 0f00000000;
	mov.f32 	%f69, %f68;
	@%p1 bra 	$L__BB2_3;

	mul.lo.s32 	%r3, %r1, %r8;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r53, %r2;

$L__BB2_2:
	add.s32 	%r9, %r53, %r3;
	mul.wide.s32 	%rd7, %r9, 4;
	add.s64 	%rd6, %rd1, %rd7;
	// begin inline asm
	ld.global.nc.f32 %f17, [%rd6];
	// end inline asm
	add.f32 	%f68, %f68, %f17;
	// begin inline asm
	ld.global.nc.f32 %f18, [%rd6];
	// end inline asm
	// begin inline asm
	ld.global.nc.f32 %f19, [%rd6];
	// end inline asm
	fma.rn.f32 	%f69, %f18, %f19, %f69;
	add.s32 	%r53, %r53, %r4;
	setp.lt.s32 	%p2, %r53, %r8;
	@%p2 bra 	$L__BB2_2;

$L__BB2_3:
	shr.s32 	%r27, %r2, 31;
	shr.u32 	%r28, %r27, 27;
	add.s32 	%r29, %r2, %r28;
	shr.s32 	%r7, %r29, 5;
	// begin inline asm
	mov.u32 %r10, %laneid;
	// end inline asm
	mov.u32 	%r12, 1;
	mov.u32 	%r25, 31;
	mov.u32 	%r26, -1;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f68, %r12, %r25, %r26;  @p add.f32 r0, r0, %f68;  mov.f32 %f20, r0;}
	// end inline asm
	mov.u32 	%r15, 2;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f20, %r15, %r25, %r26;  @p add.f32 r0, r0, %f20;  mov.f32 %f23, r0;}
	// end inline asm
	mov.u32 	%r18, 4;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f23, %r18, %r25, %r26;  @p add.f32 r0, r0, %f23;  mov.f32 %f26, r0;}
	// end inline asm
	mov.u32 	%r21, 8;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f26, %r21, %r25, %r26;  @p add.f32 r0, r0, %f26;  mov.f32 %f29, r0;}
	// end inline asm
	mov.u32 	%r24, 16;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f29, %r24, %r25, %r26;  @p add.f32 r0, r0, %f29;  mov.f32 %f70, r0;}
	// end inline asm
	setp.ne.s32 	%p3, %r10, 0;
	@%p3 bra 	$L__BB2_5;

	shl.b32 	%r30, %r7, 2;
	mov.u32 	%r31, _ZZ24RowwiseMomentsCUDAKernelE9m_storage;
	add.s32 	%r32, %r31, %r30;
	st.shared.f32 	[%r32+4], %f70;

$L__BB2_5:
	bar.sync 	0;
	setp.ne.s32 	%p4, %r2, 0;
	@%p4 bra 	$L__BB2_7;

	ld.shared.f32 	%f35, [_ZZ24RowwiseMomentsCUDAKernelE9m_storage+8];
	add.f32 	%f36, %f70, %f35;
	ld.shared.f32 	%f37, [_ZZ24RowwiseMomentsCUDAKernelE9m_storage+12];
	add.f32 	%f38, %f36, %f37;
	ld.shared.f32 	%f39, [_ZZ24RowwiseMomentsCUDAKernelE9m_storage+16];
	add.f32 	%f70, %f38, %f39;

$L__BB2_7:
	// begin inline asm
	mov.u32 %r33, %laneid;
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f69, %r12, %r25, %r26;  @p add.f32 r0, r0, %f69;  mov.f32 %f40, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f40, %r15, %r25, %r26;  @p add.f32 r0, r0, %f40;  mov.f32 %f43, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f43, %r18, %r25, %r26;  @p add.f32 r0, r0, %f43;  mov.f32 %f46, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f46, %r21, %r25, %r26;  @p add.f32 r0, r0, %f46;  mov.f32 %f49, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f49, %r24, %r25, %r26;  @p add.f32 r0, r0, %f49;  mov.f32 %f71, r0;}
	// end inline asm
	setp.ne.s32 	%p5, %r33, 0;
	@%p5 bra 	$L__BB2_9;

	shl.b32 	%r50, %r7, 2;
	mov.u32 	%r51, _ZZ24RowwiseMomentsCUDAKernelE9v_storage;
	add.s32 	%r52, %r51, %r50;
	st.shared.f32 	[%r52+4], %f71;

$L__BB2_9:
	bar.sync 	0;
	@%p4 bra 	$L__BB2_11;

	ld.shared.f32 	%f55, [_ZZ24RowwiseMomentsCUDAKernelE9v_storage+8];
	add.f32 	%f56, %f71, %f55;
	ld.shared.f32 	%f57, [_ZZ24RowwiseMomentsCUDAKernelE9v_storage+12];
	add.f32 	%f58, %f56, %f57;
	ld.shared.f32 	%f59, [_ZZ24RowwiseMomentsCUDAKernelE9v_storage+16];
	add.f32 	%f71, %f58, %f59;

$L__BB2_11:
	@%p4 bra 	$L__BB2_13;

	cvt.rn.f32.s32 	%f60, %r8;
	rcp.rn.f32 	%f61, %f60;
	mul.f32 	%f62, %f61, %f70;
	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f32 	[%rd10], %f62;
	mul.f32 	%f63, %f62, %f62;
	mul.f32 	%f64, %f61, %f71;
	sub.f32 	%f65, %f64, %f63;
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.f32 	[%rd12], %f65;

$L__BB2_13:
	ret;

}
	// .globl	ComputeInternalGradientsCUDAKernel
.visible .entry ComputeInternalGradientsCUDAKernel(
	.param .u32 ComputeInternalGradientsCUDAKernel_param_0,
	.param .u64 ComputeInternalGradientsCUDAKernel_param_1,
	.param .u64 ComputeInternalGradientsCUDAKernel_param_2,
	.param .u64 ComputeInternalGradientsCUDAKernel_param_3,
	.param .u64 ComputeInternalGradientsCUDAKernel_param_4,
	.param .u64 ComputeInternalGradientsCUDAKernel_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<67>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<17>;
	// demoted variable
	.shared .align 4 .b8 _ZZ34ComputeInternalGradientsCUDAKernelE10ds_storage[24];
	// demoted variable
	.shared .align 4 .b8 _ZZ34ComputeInternalGradientsCUDAKernelE10db_storage[24];

	ld.param.u32 	%r8, [ComputeInternalGradientsCUDAKernel_param_0];
	ld.param.u64 	%rd1, [ComputeInternalGradientsCUDAKernel_param_1];
	ld.param.u64 	%rd2, [ComputeInternalGradientsCUDAKernel_param_2];
	ld.param.u64 	%rd3, [ComputeInternalGradientsCUDAKernel_param_3];
	ld.param.u64 	%rd4, [ComputeInternalGradientsCUDAKernel_param_4];
	ld.param.u64 	%rd5, [ComputeInternalGradientsCUDAKernel_param_5];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r2, %r8;
	mov.f32 	%f63, 0f00000000;
	mov.f32 	%f64, %f63;
	@%p1 bra 	$L__BB3_3;

	mul.lo.s32 	%r3, %r1, %r8;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r53, %r2;

$L__BB3_2:
	add.s32 	%r9, %r53, %r3;
	mul.wide.s32 	%rd10, %r9, 4;
	add.s64 	%rd6, %rd1, %rd10;
	// begin inline asm
	ld.global.nc.f32 %f17, [%rd6];
	// end inline asm
	mul.wide.s32 	%rd11, %r53, 4;
	add.s64 	%rd9, %rd3, %rd11;
	// begin inline asm
	ld.global.nc.f32 %f18, [%rd9];
	// end inline asm
	fma.rn.f32 	%f63, %f17, %f18, %f63;
	add.s64 	%rd8, %rd2, %rd10;
	// begin inline asm
	ld.global.nc.f32 %f19, [%rd8];
	// end inline asm
	// begin inline asm
	ld.global.nc.f32 %f20, [%rd9];
	// end inline asm
	fma.rn.f32 	%f64, %f19, %f20, %f64;
	add.s32 	%r53, %r53, %r4;
	setp.lt.s32 	%p2, %r53, %r8;
	@%p2 bra 	$L__BB3_2;

$L__BB3_3:
	shr.s32 	%r27, %r2, 31;
	shr.u32 	%r28, %r27, 27;
	add.s32 	%r29, %r2, %r28;
	shr.s32 	%r7, %r29, 5;
	// begin inline asm
	mov.u32 %r10, %laneid;
	// end inline asm
	mov.u32 	%r12, 1;
	mov.u32 	%r25, 31;
	mov.u32 	%r26, -1;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f63, %r12, %r25, %r26;  @p add.f32 r0, r0, %f63;  mov.f32 %f21, r0;}
	// end inline asm
	mov.u32 	%r15, 2;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f21, %r15, %r25, %r26;  @p add.f32 r0, r0, %f21;  mov.f32 %f24, r0;}
	// end inline asm
	mov.u32 	%r18, 4;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f24, %r18, %r25, %r26;  @p add.f32 r0, r0, %f24;  mov.f32 %f27, r0;}
	// end inline asm
	mov.u32 	%r21, 8;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f27, %r21, %r25, %r26;  @p add.f32 r0, r0, %f27;  mov.f32 %f30, r0;}
	// end inline asm
	mov.u32 	%r24, 16;
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f30, %r24, %r25, %r26;  @p add.f32 r0, r0, %f30;  mov.f32 %f65, r0;}
	// end inline asm
	setp.ne.s32 	%p3, %r10, 0;
	@%p3 bra 	$L__BB3_5;

	shl.b32 	%r30, %r7, 2;
	mov.u32 	%r31, _ZZ34ComputeInternalGradientsCUDAKernelE10ds_storage;
	add.s32 	%r32, %r31, %r30;
	st.shared.f32 	[%r32+4], %f65;

$L__BB3_5:
	bar.sync 	0;
	setp.ne.s32 	%p4, %r2, 0;
	@%p4 bra 	$L__BB3_7;

	ld.shared.f32 	%f36, [_ZZ34ComputeInternalGradientsCUDAKernelE10ds_storage+8];
	add.f32 	%f37, %f65, %f36;
	ld.shared.f32 	%f38, [_ZZ34ComputeInternalGradientsCUDAKernelE10ds_storage+12];
	add.f32 	%f39, %f37, %f38;
	ld.shared.f32 	%f40, [_ZZ34ComputeInternalGradientsCUDAKernelE10ds_storage+16];
	add.f32 	%f65, %f39, %f40;

$L__BB3_7:
	// begin inline asm
	mov.u32 %r33, %laneid;
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f64, %r12, %r25, %r26;  @p add.f32 r0, r0, %f64;  mov.f32 %f41, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f41, %r15, %r25, %r26;  @p add.f32 r0, r0, %f41;  mov.f32 %f44, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f44, %r18, %r25, %r26;  @p add.f32 r0, r0, %f44;  mov.f32 %f47, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f47, %r21, %r25, %r26;  @p add.f32 r0, r0, %f47;  mov.f32 %f50, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .f32 r0;  .reg .pred p;  shfl.sync.down.b32 r0|p, %f50, %r24, %r25, %r26;  @p add.f32 r0, r0, %f50;  mov.f32 %f66, r0;}
	// end inline asm
	setp.ne.s32 	%p5, %r33, 0;
	@%p5 bra 	$L__BB3_9;

	shl.b32 	%r50, %r7, 2;
	mov.u32 	%r51, _ZZ34ComputeInternalGradientsCUDAKernelE10db_storage;
	add.s32 	%r52, %r51, %r50;
	st.shared.f32 	[%r52+4], %f66;

$L__BB3_9:
	bar.sync 	0;
	@%p4 bra 	$L__BB3_11;

	ld.shared.f32 	%f56, [_ZZ34ComputeInternalGradientsCUDAKernelE10db_storage+8];
	add.f32 	%f57, %f66, %f56;
	ld.shared.f32 	%f58, [_ZZ34ComputeInternalGradientsCUDAKernelE10db_storage+12];
	add.f32 	%f59, %f57, %f58;
	ld.shared.f32 	%f60, [_ZZ34ComputeInternalGradientsCUDAKernelE10db_storage+16];
	add.f32 	%f66, %f59, %f60;

$L__BB3_11:
	@%p4 bra 	$L__BB3_13;

	cvta.to.global.u64 	%rd12, %rd4;
	mul.wide.s32 	%rd13, %r1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f65;
	cvta.to.global.u64 	%rd15, %rd5;
	add.s64 	%rd16, %rd15, %rd13;
	st.global.f32 	[%rd16], %f66;

$L__BB3_13:
	ret;

}
	// .globl	ComputeFusedParamsCUDAKernel
.visible .entry ComputeFusedParamsCUDAKernel(
	.param .u32 ComputeFusedParamsCUDAKernel_param_0,
	.param .u32 ComputeFusedParamsCUDAKernel_param_1,
	.param .u64 ComputeFusedParamsCUDAKernel_param_2,
	.param .u64 ComputeFusedParamsCUDAKernel_param_3,
	.param .u64 ComputeFusedParamsCUDAKernel_param_4,
	.param .u64 ComputeFusedParamsCUDAKernel_param_5,
	.param .u64 ComputeFusedParamsCUDAKernel_param_6,
	.param .u64 ComputeFusedParamsCUDAKernel_param_7,
	.param .u64 ComputeFusedParamsCUDAKernel_param_8,
	.param .u64 ComputeFusedParamsCUDAKernel_param_9
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<28>;


	ld.param.u32 	%r3, [ComputeFusedParamsCUDAKernel_param_0];
	ld.param.u32 	%r2, [ComputeFusedParamsCUDAKernel_param_1];
	ld.param.u64 	%rd3, [ComputeFusedParamsCUDAKernel_param_2];
	ld.param.u64 	%rd4, [ComputeFusedParamsCUDAKernel_param_3];
	ld.param.u64 	%rd5, [ComputeFusedParamsCUDAKernel_param_4];
	ld.param.u64 	%rd6, [ComputeFusedParamsCUDAKernel_param_5];
	ld.param.u64 	%rd7, [ComputeFusedParamsCUDAKernel_param_6];
	ld.param.u64 	%rd8, [ComputeFusedParamsCUDAKernel_param_7];
	ld.param.u64 	%rd9, [ComputeFusedParamsCUDAKernel_param_8];
	ld.param.u64 	%rd10, [ComputeFusedParamsCUDAKernel_param_9];
	mov.u32 	%r4, %ctaid.x;
	shl.b32 	%r5, %r4, 7;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r1, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB4_3;

	cvta.to.global.u64 	%rd11, %rd6;
	cvt.rn.f32.s32 	%f2, %r2;
	rcp.rn.f32 	%f3, %f2;
	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd12, %rd4;
	mul.wide.s32 	%rd13, %r1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f4, [%rd14];
	rcp.rn.f32 	%f1, %f4;
	add.s64 	%rd15, %rd11, %rd13;
	cvta.to.global.u64 	%rd16, %rd3;
	add.s64 	%rd2, %rd16, %rd13;
	ld.global.f32 	%f5, [%rd2];
	ld.global.f32 	%f6, [%rd15];
	mul.f32 	%f7, %f6, %f5;
	cvta.to.global.u64 	%rd17, %rd5;
	add.s64 	%rd18, %rd17, %rd13;
	ld.global.f32 	%f8, [%rd18];
	sub.f32 	%f9, %f7, %f8;
	mul.f32 	%f10, %f1, %f1;
	mul.f32 	%f11, %f1, %f10;
	mul.f32 	%f12, %f11, %f9;
	mul.f32 	%f13, %f3, %f12;
	cvta.to.global.u64 	%rd19, %rd7;
	add.s64 	%rd20, %rd19, %rd13;
	st.global.f32 	[%rd20], %f1;
	cvta.to.global.u64 	%rd21, %rd8;
	add.s64 	%rd22, %rd21, %rd13;
	st.global.f32 	[%rd22], %f13;
	ld.global.f32 	%f14, [%rd2];
	ld.global.f32 	%f15, [%rd15];
	mul.f32 	%f16, %f1, %f15;
	mul.f32 	%f17, %f3, %f16;
	fma.rn.f32 	%f18, %f13, %f14, %f17;
	neg.f32 	%f19, %f18;
	cvta.to.global.u64 	%rd23, %rd9;
	add.s64 	%rd24, %rd23, %rd13;
	st.global.f32 	[%rd24], %f19;
	setp.eq.s64 	%p2, %rd10, 0;
	@%p2 bra 	$L__BB4_3;

	ld.global.f32 	%f20, [%rd2];
	mul.f32 	%f21, %f1, %f20;
	neg.f32 	%f22, %f21;
	cvta.to.global.u64 	%rd25, %rd10;
	shl.b64 	%rd26, %rd1, 2;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.f32 	[%rd27], %f22;

$L__BB4_3:
	ret;

}
	// .globl	LayerNormBackwardCUDAKernel
.visible .entry LayerNormBackwardCUDAKernel(
	.param .u32 LayerNormBackwardCUDAKernel_param_0,
	.param .u32 LayerNormBackwardCUDAKernel_param_1,
	.param .u64 LayerNormBackwardCUDAKernel_param_2,
	.param .u64 LayerNormBackwardCUDAKernel_param_3,
	.param .u64 LayerNormBackwardCUDAKernel_param_4,
	.param .u64 LayerNormBackwardCUDAKernel_param_5,
	.param .u64 LayerNormBackwardCUDAKernel_param_6,
	.param .u64 LayerNormBackwardCUDAKernel_param_7,
	.param .u64 LayerNormBackwardCUDAKernel_param_8
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<25>;


	ld.param.u32 	%r3, [LayerNormBackwardCUDAKernel_param_0];
	ld.param.u32 	%r2, [LayerNormBackwardCUDAKernel_param_1];
	ld.param.u64 	%rd1, [LayerNormBackwardCUDAKernel_param_2];
	ld.param.u64 	%rd2, [LayerNormBackwardCUDAKernel_param_3];
	ld.param.u64 	%rd3, [LayerNormBackwardCUDAKernel_param_4];
	ld.param.u64 	%rd4, [LayerNormBackwardCUDAKernel_param_5];
	ld.param.u64 	%rd5, [LayerNormBackwardCUDAKernel_param_6];
	ld.param.u64 	%rd6, [LayerNormBackwardCUDAKernel_param_7];
	ld.param.u64 	%rd7, [LayerNormBackwardCUDAKernel_param_8];
	mov.u32 	%r4, %ctaid.x;
	shl.b32 	%r5, %r4, 7;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r1, %r5, %r6;
	mul.lo.s32 	%r7, %r2, %r3;
	setp.ge.s32 	%p1, %r1, %r7;
	@%p1 bra 	$L__BB5_2;

	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	div.s32 	%r8, %r1, %r2;
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r8, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f32 	%f1, [%rd13];
	ld.global.f32 	%f2, [%rd10];
	mul.f32 	%f3, %f2, %f1;
	mul.lo.s32 	%r9, %r8, %r2;
	sub.s32 	%r10, %r1, %r9;
	cvta.to.global.u64 	%rd14, %rd3;
	mul.wide.s32 	%rd15, %r10, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.f32 	%f4, [%rd16];
	cvta.to.global.u64 	%rd17, %rd2;
	add.s64 	%rd18, %rd17, %rd9;
	cvta.to.global.u64 	%rd19, %rd5;
	add.s64 	%rd20, %rd19, %rd12;
	ld.global.f32 	%f5, [%rd20];
	ld.global.f32 	%f6, [%rd18];
	mul.f32 	%f7, %f6, %f5;
	fma.rn.f32 	%f8, %f3, %f4, %f7;
	cvta.to.global.u64 	%rd21, %rd6;
	add.s64 	%rd22, %rd21, %rd12;
	ld.global.f32 	%f9, [%rd22];
	add.f32 	%f10, %f9, %f8;
	cvta.to.global.u64 	%rd23, %rd7;
	add.s64 	%rd24, %rd23, %rd9;
	st.global.f32 	[%rd24], %f10;

$L__BB5_2:
	ret;

}

